\section{\texorpdfstring{\texttt{fitr.agents}}{fitr.agents}}\label{fitr.agents}

A modular way to build and test reinforcement learning agents.

There are three main submodules:

\begin{itemize}
\tightlist
\item
  \texttt{fitr.agents.policies}: which describe a class of functions
  essentially representing \(f:\mathcal X \to \mathcal U\)
\item
  \texttt{fitr.agents.value\_functions}: which describe a class of
  functions essentially representing
  \(\mathcal V: \mathcal X \to \mathbb R\) and/or
  \(\mathcal Q: \mathcal Q \times \mathcal U \to \mathbb R\)
\item
  \texttt{fitr.agents.agents}: classes of agents that are combinations
  of policies and value functions, along with some convenience functions
  for generating data from \texttt{fitr.environments.Graph}
  environments.
\end{itemize}

\subsection{SoftmaxPolicy}\label{softmaxpolicy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.SoftmaxPolicy()}
\end{Highlighting}
\end{Shaded}

Action selection by sampling from a multinomial whose parameters are
given by a softmax.

Action sampling is

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)).
\]

Parameters of that distribution are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState} object
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SoftmaxPolicy.action\_prob}\label{softmaxpolicy.action_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.action_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the softmax

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SoftmaxPolicy.log\_prob}\label{softmaxpolicy.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.log_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of an action \(\mathbf u\)

\[
\log p(\mathbf u|\mathbf v) = \beta \mathbf v - \log \sum_{v_i} e^{\beta \mathbf v_i}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: State vector of type \texttt{ndarray((nstates,))}
\end{itemize}

Returns:

Scalar log-probability

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SoftmaxPolicy.sample}\label{softmaxpolicy.sample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.sample(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Samples from the action distribution

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{StickySoftmaxPolicy}\label{stickysoftmaxpolicy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.StickySoftmaxPolicy()}
\end{Highlighting}
\end{Shaded}

Action selection by sampling from a multinomial whose parameters are
given by a softmax, but with accounting for the tendency to perseverate
(i.e.~choosing the previously used action without considering its
value).

Let \(\mathbf u_{t-1} = (u_{t-1}^{(i)})_{i=1}^{|\mathcal U|}\) be a one
hot vector representing the action taken at the last step, and
\(\beta^\rho\) be an inverse softmax temperature for the influence of
this last action.

Action sampling is thus:

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v, \mathbf u_{t-1})).
\]

Parameters of that distribution are

\[
p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \varsigma(\mathbf v, \mathbf u_{t-1}) = \frac{e^{\beta \mathbf v + \beta^\rho \mathbf u_{t-1}}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i + \beta^\rho u_{t-1}^{(i)}}}.
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{perseveration}: Inverse softmax temperature \(\beta^\rho\)
  capturing the tendency to repeat the last action taken.
\item
  \textbf{rng}: \texttt{np.random.RandomState} object
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{StickySoftmaxPolicy.action\_prob}\label{stickysoftmaxpolicy.action_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.action_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the softmax

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nstates,))} vector of action probabilities

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{StickySoftmaxPolicy.log\_prob}\label{stickysoftmaxpolicy.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.log_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Computes the log-probability of an action \(\mathbf u\)

\[
\log p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \big(\beta \mathbf v + \beta^\rho \mathbf u_{t-1}) - \log \sum_{v_i} e^{\beta \mathbf v_i + \beta^\rho u_{t-1}^{(i)}}
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: State vector of type \texttt{ndarray((nstates,))}
\end{itemize}

Returns:

Scalar log-probability

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{StickySoftmaxPolicy.sample}\label{stickysoftmaxpolicy.sample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.sample(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Samples from the action distribution

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nstates,))} one-hot action vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{EpsilonGreedyPolicy}\label{epsilongreedypolicy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.EpsilonGreedyPolicy()}
\end{Highlighting}
\end{Shaded}

A policy that takes the maximally valued action with probability
\(1-\epsilon\), otherwise chooses randomlyself.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{epsilon}: Probability of not taking the action with highest
  value
\item
  \textbf{rng}: \texttt{numpy.random.RandomState} object
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{EpsilonGreedyPolicy.action\_prob}\label{epsilongreedypolicy.action_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.action_prob(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Creates vector of action probabilities for e-greedy policy

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nstates,))} vector of action probabilities

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{EpsilonGreedyPolicy.sample}\label{epsilongreedypolicy.sample}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.policies.sample(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Samples from the action distribution

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nstates,))} one-hot action vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{ValueFunction}\label{valuefunction}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.ValueFunction()}
\end{Highlighting}
\end{Shaded}

A general value function object.

A value function here is task specific and consists of several
attributes:

\begin{itemize}
\tightlist
\item
  \texttt{nstates}: The number of states in the task, \(|\mathcal X|\)
\item
  \texttt{nactions}: Number of actions in the task, \(|\mathcal U|\)
\item
  \texttt{V}: State value function \(\mathbf v = \mathcal V(\mathbf x)\)
\item
  \texttt{Q}: State-action value function
  \(\mathbf Q = \mathcal Q(\mathbf x, \mathbf u)\)
\item
  \texttt{etrace}: An eligibility trace (optional)
\end{itemize}

Note that in general we rely on matrix-vector notation for value
functions, rather than function notation. Vectors in the mathematical
typesetting are by default column vectors.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Qmax}\label{valuefunction.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Qmean}\label{valuefunction.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Qx}\label{valuefunction.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.Vx}\label{valuefunction.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.uQx}\label{valuefunction.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{ValueFunction.update}\label{valuefunction.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{DummyLearner}\label{dummylearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.DummyLearner()}
\end{Highlighting}
\end{Shaded}

A critic/value function for the random learner

This class actually contributes nothing except identifying that a value
function has been chosen for an \texttt{Agent} object

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Qmax}\label{dummylearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Qmean}\label{dummylearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Qx}\label{dummylearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.Vx}\label{dummylearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.uQx}\label{dummylearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{DummyLearner.update}\label{dummylearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{InstrumentalRescorlaWagnerLearner}\label{instrumentalrescorlawagnerlearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.InstrumentalRescorlaWagnerLearner()}
\end{Highlighting}
\end{Shaded}

Learns an instrumental control policy through one-step error-driven
updates of the state-action value function

The instrumental Rescorla-Wagner rule is as follows:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, and where the reward
prediction error (RPE) is
\(\delta = (r - \mathbf u^\top \mathbf Q \mathbf x)\).

\$\$

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Qmax}\label{instrumentalrescorlawagnerlearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Qmean}\label{instrumentalrescorlawagnerlearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Qx}\label{instrumentalrescorlawagnerlearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.Vx}\label{instrumentalrescorlawagnerlearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.uQx}\label{instrumentalrescorlawagnerlearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{InstrumentalRescorlaWagnerLearner.update}\label{instrumentalrescorlawagnerlearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{QLearner}\label{qlearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.QLearner()}
\end{Highlighting}
\end{Shaded}

Learns an instrumental control policy through Q-learning

The Q-learning rule is as follows:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Qmax}\label{qlearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Qmean}\label{qlearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Qx}\label{qlearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.Vx}\label{qlearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.uQx}\label{qlearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearner.update}\label{qlearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{SARSALearner}\label{sarsalearner}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.SARSALearner()}
\end{Highlighting}
\end{Shaded}

Learns an instrumental control policy through the SARSA learning rule

The SARSA learning rule is as follows:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{env}: A \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Qmax}\label{sarsalearner.qmax}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmax(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return maximal action value for given state

\[
\max_{u_i}\mathcal Q(\mathbf x, u_i) = \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Qmean}\label{sarsalearner.qmean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qmean(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Return mean action value for given state

\[
Mean \big(\mathcal Q(\mathbf x, :)\big) = \frac{1}{|\mathcal U|} \mathbf 1^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of the maximal action value at the given state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Qx}\label{sarsalearner.qx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Qx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute action values for a given state

\[
\mathcal Q(\mathbf x, :) = \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} vector of values for actions in the given
state

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.Vx}\label{sarsalearner.vx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.Vx(}\VariableTok{self}\NormalTok{, x)}
\end{Highlighting}
\end{Shaded}

Compute value of state \(\mathbf x\)

\[
\mathcal V(\mathbf x) = \mathbf v^\top \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.uQx}\label{sarsalearner.uqx}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.uQx(}\VariableTok{self}\NormalTok{, u, x)}
\end{Highlighting}
\end{Shaded}

Compute value of taking action \(\mathbf u\) in state \(\mathbf x\)

\[
\mathcal Q(\mathbf x, \mathbf u) = \mathbf u^\top \mathbf Q \mathbf x
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

Scalar value of action \(\mathbf u\) in state \(\mathbf x\)

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSALearner.update}\label{sarsalearner.update}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.value_functions.update(}\VariableTok{self}\NormalTok{, x, u, r, x_, u_)}
\end{Highlighting}
\end{Shaded}

Updates the value function

In the context of the base \texttt{ValueFunction} class, this is merely
a placeholder. The specific update rule will depend on the specific
value function desired.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{r}: Scalar reward
\item
  \textbf{x\_}: \texttt{ndarray((nstates,))} one-hot next-state vector
\item
  \textbf{u\_}: \texttt{ndarray((nactions,))} one-hot next-action vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{Agent}\label{agent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.Agent()}
\end{Highlighting}
\end{Shaded}

Base class for synthetic RL agents.

Arguments:

meta : List of metadata of arbitrary type. e.g.~labels, covariates, etc.
params : List of parameters for the agent. Should be filled for specific
agent.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Agent.action}\label{agent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Agent.learning}\label{agent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{Agent.reset\_trace}\label{agent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{BanditAgent}\label{banditagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.BanditAgent()}
\end{Highlighting}
\end{Shaded}

A base class for agents in bandit tasks (i.e.~with one step).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.action}\label{banditagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.generate\_data}\label{banditagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.learning}\label{banditagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.log\_prob}\label{banditagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{BanditAgent.reset\_trace}\label{banditagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{MDPAgent}\label{mdpagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.MDPAgent()}
\end{Highlighting}
\end{Shaded}

A base class for agents that operate on MDPs.

This mainly has implications for generating data.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.action}\label{mdpagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.generate\_data}\label{mdpagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.learning}\label{mdpagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{MDPAgent.reset\_trace}\label{mdpagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RandomBanditAgent}\label{randombanditagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RandomBanditAgent()}
\end{Highlighting}
\end{Shaded}

An agent that simply selects random actions at each trial

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.action}\label{randombanditagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.generate\_data}\label{randombanditagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.learning}\label{randombanditagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.log\_prob}\label{randombanditagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomBanditAgent.reset\_trace}\label{randombanditagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RandomMDPAgent}\label{randommdpagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RandomMDPAgent()}
\end{Highlighting}
\end{Shaded}

An agent that simply selects random actions at each trial

\subsection{Notes}\label{notes}

This has been specified as an \texttt{OnPolicyAgent} arbitrarily.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.action}\label{randommdpagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.generate\_data}\label{randommdpagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.learning}\label{randommdpagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RandomMDPAgent.reset\_trace}\label{randommdpagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{SARSASoftmaxAgent}\label{sarsasoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.SARSASoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An agent that uses the SARSA learning rule and a softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is SARSA:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.action}\label{sarsasoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.generate\_data}\label{sarsasoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.learning}\label{sarsasoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSASoftmaxAgent.reset\_trace}\label{sarsasoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{SARSAStickySoftmaxAgent}\label{sarsastickysoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.SARSAStickySoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An agent that uses the SARSA learning rule and a sticky softmax policy

The sticky softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \varsigma(\mathbf v, \mathbf u_{t-1}) = \frac{e^{\beta \mathbf v + \beta^\rho \mathbf u_{t-1}}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i + \beta^\rho u_{t-1}^{(i)}}}.
\]

The value function is SARSA:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
We have also included an eligibility trace \(\mathbf z\) defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{perseveration}: Perseveration parameter \(\beta^\rho\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.action}\label{sarsastickysoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.generate\_data}\label{sarsastickysoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.learning}\label{sarsastickysoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{SARSAStickySoftmaxAgent.reset\_trace}\label{sarsastickysoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{QLearningSoftmaxAgent}\label{qlearningsoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.QLearningSoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An agent that uses the Q-learning rule and a softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is Q-learning:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf z,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r + \gamma \max_{\mathbf u'} \mathbf u'^\top \mathbf Q \mathbf x' - \mathbf u^\top \mathbf Q \mathbf x)\).
The eligibility trace \(\mathbf z\) is defined as

\[
\mathbf z = \mathbf u \mathbf x^\top +  \gamma \lambda \mathbf z
\]

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{discount\_factor}: Discount factor \(\gamma\)
\item
  \textbf{trace\_decay}: Eligibility trace decay \(\lambda\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.action}\label{qlearningsoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.generate\_data}\label{qlearningsoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a Markov
Decision Process (MDP) task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.learning}\label{qlearningsoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{QLearningSoftmaxAgent.reset\_trace}\label{qlearningsoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RWSoftmaxAgent}\label{rwsoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RWSoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An instrumental Rescorla-Wagner agent with a softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is the Rescorla-Wagner learning rule:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r - \mathbf u^\top \mathbf Q \mathbf x)\).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.action}\label{rwsoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.generate\_data}\label{rwsoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.learning}\label{rwsoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.log\_prob}\label{rwsoftmaxagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgent.reset\_trace}\label{rwsoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RWStickySoftmaxAgent}\label{rwstickysoftmaxagent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RWStickySoftmaxAgent()}
\end{Highlighting}
\end{Shaded}

An instrumental Rescorla-Wagner agent with a `sticky' softmax policy

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v, \mathbf u_{t-1})).
\]

whose parameters are

\[
p(\mathbf u|\mathbf v, \mathbf u_{t-1}) = \varsigma(\mathbf v, \mathbf u_{t-1}) = \frac{e^{\beta \mathbf v + \beta^\rho \mathbf u_{t-1}}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i + \beta^\rho u_{t-1}^{(i)}}}.
\]

The value function is the Rescorla-Wagner learning rule:

\[
\mathbf Q \gets \mathbf Q + \alpha \big(r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (r - \mathbf u^\top \mathbf Q \mathbf x)\).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{perseveration}: Perseveration parameter \(\beta^ ho\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.action}\label{rwstickysoftmaxagent.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.generate\_data}\label{rwstickysoftmaxagent.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.learning}\label{rwstickysoftmaxagent.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.log\_prob}\label{rwstickysoftmaxagent.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWStickySoftmaxAgent.reset\_trace}\label{rwstickysoftmaxagent.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{RWSoftmaxAgentRewardSensitivity}\label{rwsoftmaxagentrewardsensitivity}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.RWSoftmaxAgentRewardSensitivity()}
\end{Highlighting}
\end{Shaded}

An instrumental Rescorla-Wagner agent with a softmax policy, whose
experienced reward is scaled by a factor \(\rho\).

The softmax policy selects actions from a multinomial

\[
\mathbf u \sim \mathrm{Multinomial}(1, \mathbf p=\varsigma(\mathbf v)),
\]

whose parameters are

\[
p(\mathbf u|\mathbf v) = \varsigma(\mathbf v) = \frac{e^{\beta \mathbf v}}{\sum_{i}^{|\mathbf v|} e^{\beta v_i}}.
\]

The value function is the Rescorla-Wagner learning rule with scaled
reward \(\rho r\):

\[
\mathbf Q \gets \mathbf Q + \alpha \big(\rho r - \mathbf u^\top \mathbf Q \mathbf x \big) \mathbf u \mathbf x^\top,
\]

where \(0 < \alpha < 1\) is the learning rate, \(0 \leq \gamma \leq 1\)
is a discount factor, and where the reward prediction error (RPE) is
\(\delta = (\rho r - \mathbf u^\top \mathbf Q \mathbf x)\).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{task}: \texttt{fitr.environments.Graph}
\item
  \textbf{learning\_rate}: Learning rate \(\alpha\)
\item
  \textbf{inverse\_softmax\_temp}: Inverse softmax temperature \(\beta\)
\item
  \textbf{reward\_sensitivity}: Reward sensitivity parameter \(\rho\)
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.action}\label{rwsoftmaxagentrewardsensitivity.action}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.action(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Selects an action given the current state of environment.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.generate\_data}\label{rwsoftmaxagentrewardsensitivity.generate_data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.generate_data(}\VariableTok{self}\NormalTok{, ntrials)}
\end{Highlighting}
\end{Shaded}

For the parent agent, this function generates data from a bandit task

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{ntrials}: \texttt{int} number of trials
\end{itemize}

Returns:

\texttt{fitr.data.BehaviouralData}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.learning}\label{rwsoftmaxagentrewardsensitivity.learning}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.learning(}\VariableTok{self}\NormalTok{, state, action, reward, next_state, next_action)}
\end{Highlighting}
\end{Shaded}

Updates the model's parameters.

The implementation will vary depending on the type of agent and
environment.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{action}: \texttt{ndarray((nactions,))} one-hot action vector
\item
  \textbf{reward}: scalar reward
\item
  \textbf{next\_state}: \texttt{ndarray((nstates,))} one-hot next-state
  vector
\item
  \textbf{next\_action}: \texttt{ndarray((nactions,))} one-hot action
  vector
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.log\_prob}\label{rwsoftmaxagentrewardsensitivity.log_prob}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.log_prob(}\VariableTok{self}\NormalTok{, state)}
\end{Highlighting}
\end{Shaded}

Computes the log-likelihood over actions for a given state under the
present agent parameters.

Presently this only works for the state-action value function. In all
other cases, you should define your own log-likelihood function.
However, this can be used as a template.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{state}: \texttt{ndarray((nstates,))} one-hot state vector
\end{itemize}

Returns:

\texttt{ndarray((nactions,))} log-likelihood vector

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{RWSoftmaxAgentRewardSensitivity.reset\_trace}\label{rwsoftmaxagentrewardsensitivity.reset_trace}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.agents.agents.reset_trace(}\VariableTok{self}\NormalTok{, x, u}\OperatorTok{=}\VariableTok{None}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For agents with eligibility traces, this resets the eligibility trace
(for episodic tasks)

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{x}: \texttt{ndarray((nstates,))} one-hot state vector
\item
  \textbf{u}: \texttt{ndarray((nactions,))} one-hot action vector
  (optional)
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
